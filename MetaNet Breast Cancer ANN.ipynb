{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D, InputLayer\n",
    "import PIL\n",
    "import os\n",
    "from keras.optimizers import Adam\n",
    "from keras import applications\n",
    "from keras.layers import Input\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from shutil import copyfile, copy2\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "# After uploading a file named \"Set1&2.zip\" to Google Colaboratory, load it. Set1&2 has roughly 40,000 images. 35,000 in train, 5,000 in test. \n",
    "file_name = \"Set1&2.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "\tzip.extractall()\n",
    "\tprint(\"Done\")\n",
    "  \n",
    "#with augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "generator = ImageDataGenerator(rescale=1./255,\n",
    "                               zoom_range=0.2, # Zoom in by 20% max\n",
    "                               horizontal_flip=True, # Allo horizontal flipping\n",
    "                               fill_mode='nearest')\n",
    "image_shape = (50,50,3)\n",
    "\n",
    "generator_noaug = ImageDataGenerator(rescale=1./255)\n",
    "# Training Dataset (With Shuffle)\n",
    "pathtrain = \"/content/Set1&2/Train\"\n",
    "firsttrain = generator.flow_from_directory(\n",
    "        pathtrain,\n",
    "        target_size=image_shape[:2],\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n",
    "# Test Dataset (Without Shuffl)\n",
    "pathtest = \"/content/Set1&2/Test\"\n",
    "firsttest = generator_noaug.flow_from_directory(\n",
    "        pathtest,\n",
    "        target_size=image_shape[:2],\n",
    "        batch_size=64,\n",
    "        shuffle=False)\n",
    "\n",
    "# Assign Class Weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weight = compute_class_weight(class_weight='balanced',\n",
    "                                    classes=np.unique(firsttrain.classes),\n",
    "                                    y=firsttrain.classes)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same training dataset but without shuffle (so wrong images can be identified)\n",
    "image_shape = (50,50,3)\n",
    "validate_first_train = generator.flow_from_directory(\n",
    "        pathtrain,\n",
    "        target_size=image_shape[:2],\n",
    "        batch_size=64,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories to store images for MetaNet training\n",
    "# define the names of the directories to be created\n",
    "path1 = \"/content/Resnet2\"\n",
    "path2 = \"/content/Resnet2/Benign\"\n",
    "path3 = \"/content/Resnet2/Malignant\"\n",
    "\n",
    "path4 = \"/content/Resnet3\"\n",
    "path5 = \"/content/Resnet3/Benign\"\n",
    "path6 = \"/content/Resnet3/Malignant\"\n",
    "\n",
    "path7 = \"/content/Resnet4\"\n",
    "path8 = \"/content/Resnet4/Benign\"\n",
    "path9 = \"/content/Resnet4/Malignant\"\n",
    "\n",
    "# ONLY RUN THIS CODE ONCE BC It makes the directories\n",
    "import os\n",
    "import shutil\n",
    "shutil.rmtree(path2,path3)\n",
    "try:\n",
    "  os.mkdir(path1)\n",
    "  os.mkdir(path2)\n",
    "  os.mkdir(path3)\n",
    "  os.mkdir(path4)\n",
    "  os.mkdir(path5)\n",
    "  os.mkdir(path6)  \n",
    "  os.mkdir(path7)\n",
    "  os.mkdir(path8)\n",
    "  os.mkdir(path9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directories for the Ensemble Training\n",
    "path1b = \"/content/Resnet2b\"\n",
    "path2b = \"/content/Resnet2b/Benign\"\n",
    "path3b = \"/content/Resnet2b/Malignant\"\n",
    "\n",
    "path4b = \"/content/Resnet3b\"\n",
    "path5b = \"/content/Resnet3b/Benign\"\n",
    "path6b = \"/content/Resnet3b/Malignant\"\n",
    "\n",
    "path7b = \"/content/Resnet4b\"\n",
    "path8b = \"/content/Resnet4b/Benign\"\n",
    "path9b = \"/content/Resnet4b/Malignant\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "#shutil.rmtree(path2,path3)\n",
    "try:\n",
    "  os.mkdir(path1b)\n",
    "  os.mkdir(path2b)\n",
    "  os.mkdir(path3b)\n",
    "  os.mkdir(path4b)\n",
    "  os.mkdir(path5b)\n",
    "  os.mkdir(path6b)  \n",
    "  os.mkdir(path7b)\n",
    "  os.mkdir(path8b)\n",
    "  os.mkdir(path9b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all MetaNet Constituent Models (after training)\n",
    "from keras.models import load_model\n",
    "resnet1 = load_model('resnet1.h5')\n",
    "resnet2 = load_model('resnet2_finetune.h5')\n",
    "resnet3 = load_model('resnet3_finetune.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find ROC Value of MetaNet\n",
    "fpr_meta, tpr_meta, thresholds_meta = metrics.roc_curve(y_test_vals2,final_predictions_ann)\n",
    "print(metrics.auc(fpr_meta, tpr_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_pred_vals = resnet1_refinetune.predict_generator(validate_first_train,validate_first_train.samples/validate_first_train.batch_size)\n",
    "model2_pred_vals = resnet2.predict_generator(validate_first_train,validate_first_train.samples/validate_first_train.batch_size)\n",
    "model3_pred_vals = resnet3.predict_generator(validate_first_train,validate_first_train.samples/validate_first_train.batch_size)\n",
    "model4_pred_vals = resnet1.predict_generator(validate_first_train,validate_first_train.samples/validate_first_train.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_pred_vals_0 = [i[0] for i in model1_pred_vals]\n",
    "model1_pred_vals_1 = [i[1] for i in model1_pred_vals]\n",
    "\n",
    "reshaped_model2_pred_vals_0 = [i[0] for i in model2_pred_vals]\n",
    "reshaped_model2_pred_vals_1 = [i[1] for i in model2_pred_vals]\n",
    "\n",
    "reshaped_model3_pred_vals_0 = [i[0] for i in model3_pred_vals]\n",
    "reshaped_model3_pred_vals_1 = [i[1] for i in model3_pred_vals]\n",
    "\n",
    "reshaped_model4_pred_vals_0 = [i[0] for i in model4_pred_vals]\n",
    "reshaped_model4_pred_vals_1 = [i[1] for i in model4_pred_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ANN\n",
    "\n",
    "import pandas as pd\n",
    "true_train_classes=validate_first_train.classes\n",
    "dataset3_train = pd.DataFrame({'first0': model1_pred_vals_0,\n",
    "                        'first1': model1_pred_vals_1,\n",
    "                        'inverse0': reshaped_model2_pred_vals_0,\n",
    "                        'inverse1': reshaped_model2_pred_vals_1,\n",
    "                        'invverse0': reshaped_model3_pred_vals_0,\n",
    "                        'invverse1': reshaped_model3_pred_vals_1,\n",
    "                        'correct':true_train_classes}, \n",
    "                       index=np.arange(len(true_train_classes)),columns=['first0',\n",
    "                                                                   'first1',\n",
    "                                                                   'inverse0',\n",
    "                                                                   'inverse1',\n",
    "                                                                   'invverse0',\n",
    "                                                                   'invverse1',\n",
    "                                                                   'correct'])\n",
    "\n",
    "# Test ANN\n",
    "import pandas as pd\n",
    "true_train_classes=validate_first_train.classes\n",
    "dataset4_train = pd.DataFrame({'first0': model1_pred_vals_0,\n",
    "                        'first1': model1_pred_vals_1,\n",
    "                        'inverse0': reshaped_model2_pred_vals_0,\n",
    "                        'inverse1': reshaped_model2_pred_vals_1,\n",
    "                        'invverse0': reshaped_model3_pred_vals_0,\n",
    "                        'invverse1': reshaped_model3_pred_vals_1,\n",
    "                        'invvverse0': reshaped_model4_pred_vals_0,\n",
    "                        'invvverse1': reshaped_model4_pred_vals_1,\n",
    "                        'correct':true_train_classes}, \n",
    "                       index=np.arange(len(true_train_classes)),columns=['first0',\n",
    "                                                                   'first1',\n",
    "                                                                   'inverse0',\n",
    "                                                                   'inverse1',\n",
    "                                                                   'invverse0',\n",
    "                                                                   'invverse1',\n",
    "                                                                   'invvverse0',\n",
    "                                                                   'invvverse1',\n",
    "                                                                   'correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vals = dataset3_train.iloc[:, :6].values\n",
    "y_train_vals = dataset3_train.iloc[:, 6].values\n",
    "\n",
    "X_train_vals10 = dataset4_train.iloc[:, :8].values\n",
    "y_train_vals10 = dataset4_train.iloc[:, 8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_train_vals, y_train_vals, test_size=0.2, random_state=91)\n",
    "y_train3_cat = np_utils.to_categorical(y_train3,2)\n",
    "y_test3_cat = np_utils.to_categorical(y_test3,2)\n",
    "\n",
    "X_train10, X_test10, y_train10, y_test10 = train_test_split(X_train_vals10, y_train_vals10, test_size=0.2, random_state=91)\n",
    "y_train3_cat10 = np_utils.to_categorical(y_train10,2)\n",
    "y_test3_cat10 = np_utils.to_categorical(y_test10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates ann_model2- all training images\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "ann_model4 = Sequential()\n",
    "\n",
    "ann_model4.add(Dense(8, input_dim=8, activation='relu'))\n",
    "ann_model4.add(Dense(256, activation='relu'))\n",
    "ann_model4.add(Dropout(0.6))\n",
    "\n",
    "ann_model4.add(Dense(256, activation='relu'))\n",
    "ann_model4.add(Dropout(0.6))\n",
    "\n",
    "# Last layer simple sigmoid function to output 0 or 1 (our label)\n",
    "ann_model4.add(Dense(2, activation='softmax'))\n",
    "\n",
    "ann_model4.compile(Adam(lr=0.01),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint_ann = ModelCheckpoint('ann_three_models4.h5', verbose=1, monitor='loss',save_best_only=True, mode='auto')  \n",
    "\n",
    "ann_model_results = ann_model4.fit(X_train10,y_train3_cat10,epochs=15,  callbacks=[checkpoint_ann], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "ann_model = Sequential()\n",
    "\n",
    "ann_model.add(Dense(6, input_dim=6, activation='relu'))\n",
    "ann_model.add(Dense(256, activation='relu'))\n",
    "ann_model.add(Dropout(0.6))\n",
    "\n",
    "ann_model.add(Dense(256, activation='relu'))\n",
    "ann_model.add(Dropout(0.6))\n",
    "\n",
    "# Last layer simple sigmoid function to output 0 or 1 (our label)\n",
    "ann_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "ann_model.compile(Adam(lr=0.01),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint_ann = ModelCheckpoint('ann_three_models.h5', verbose=1, monitor='loss',save_best_only=True, mode='auto')  \n",
    "\n",
    "ann_model_results = ann_model.fit(X_train3,y_train3_cat,epochs=15,  callbacks=[checkpoint_ann], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model4 = load_model('ann_three_models4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdtest.reset()\n",
    "model1_pred_test_vals2 = resnet1.predict_generator(thirdtest,steps=thirdtest.samples/thirdtest.batch_size)\n",
    "thirdtest.reset()\n",
    "model2_pred_test_vals2 = resnet2.predict_generator(thirdtest,steps=thirdtest.samples/thirdtest.batch_size)\n",
    "thirdtest.reset()\n",
    "\n",
    "model3_pred_test_vals2 = resnet3.predict_generator(thirdtest,steps=thirdtest.samples/thirdtest.batch_size)\n",
    "\n",
    "model1_pred_test_vals_02 = [i[0] for i in model1_pred_test_vals2]\n",
    "model1_pred_test_vals_12 = [i[1] for i in model1_pred_test_vals2]\n",
    "\n",
    "reshaped_model2_pred_test_vals_02 = [i[0] for i in model2_pred_test_vals2]\n",
    "reshaped_model2_pred_test_vals_12 = [i[1] for i in model2_pred_test_vals2]\n",
    "\n",
    "reshaped_model3_pred_test_vals_02 = [i[0] for i in model3_pred_test_vals2]\n",
    "reshaped_model3_pred_test_vals_12 = [i[1] for i in model3_pred_test_vals2]\n",
    "\n",
    "import pandas as pd\n",
    "true_test_classes2 = thirdtest.classes\n",
    "dataset32 = pd.DataFrame({'first0': model1_pred_test_vals_02,\n",
    "                        'first1': model1_pred_test_vals_12,\n",
    "                        'inverse0': reshaped_model2_pred_test_vals_02,\n",
    "                        'inverse1': reshaped_model2_pred_test_vals_12,\n",
    "                        'invverse0': reshaped_model3_pred_test_vals_02,\n",
    "                        'invverse1': reshaped_model3_pred_test_vals_12,\n",
    "                        'correct':true_test_classes2}, \n",
    "                       index=np.arange(len(true_test_classes2)),columns=['first0',\n",
    "                                                                   'first1',\n",
    "                                                                   'inverse0',\n",
    "                                                                   'inverse1',\n",
    "                                                                   'invverse0',\n",
    "                                                                   'invverse1',\n",
    "                                                                   'correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vals2 = dataset3.iloc[:, :8].values\n",
    "y_test_vals2 = dataset3.iloc[:, 8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN on firsttrain split test imgs\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "predictions_test = ann_model.predict_classes(X_test3)\n",
    "\n",
    "print(classification_report(y_test3,predictions_test))\n",
    "print(confusion_matrix(y_test3,predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN on firsttrain split test imgs\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "predictions_test = ann_model4.predict_classes(X_test10)\n",
    "\n",
    "print(classification_report(y_test10,predictions_test))\n",
    "print(confusion_matrix(y_test10,predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_ann = ann_model4.predict_classes(X_test_vals2)\n",
    "\n",
    "print(classification_report(y_test_vals2, final_predictions_ann))  \n",
    "print(confusion_matrix(y_test_vals2,final_predictions_ann))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
